# [GPT-GNN](https://arxiv.org/pdf/2006.15437.pdf)

## Key ideas
* Task-specific labeled data is usually required for GNNs which is arduous to obtain.
* Pre-training expressive GNN model on unlabeled data with self-supervision and the transfer the learned model to downstream tasks with only a few labels.
* Likelihood of graph generation into two components: attribute generation and edge generation

## Introduction
* GNNs for semi-supervised node classification, recsys and knowledge graph inference 
* Input: graph with attributes - conv filters generate node-level representations layer by layer
* Similar to BERT pre-trained model, you can train in an unlabeled corpus and then transfer the model to downstream tasks with fea labels.
* NN-generation techniques don't work for GNNs because they generate graph structure without attributes. Also their scale is limited.
<img width="547" alt="Screenshot 2022-04-04 at 13 52 33" src="https://user-images.githubusercontent.com/598891/161547763-d6b029f9-b54d-483f-8472-d2f0bd57ba30.png">
* Attribute generation and edge generation joint optimization == maximizing probability likelihood of the whoel attributed graph
* OAG: open academic grpah of 179M nodes & 2B edges successfully done.

## Preliminaries and related work 
* Assuming H_t is the node representation of node t at lth GNN layer.
* N_t are all of the source nodes of node t. E(s,t) all edges from s to t.
<img width="493" alt="Screenshot 2022-04-04 at 13 55 29" src="https://user-images.githubusercontent.com/598891/161548291-88027152-8e25-4303-bd8a-e612af667f70.png">
* Aggregate: aggregation from neighborhood information (mean, max, sum)
* Extract: neighborhood info extractor
* Variational Graph Auto-Encoders for reconstructing the graph structure.
* InfoGraph: maximizes multual information between graph-level representations from GNNs

## Generative pretraining of GNNs:
* Input to GNN: G=(V,E,X) node, edge, node-feature matrix
* GPT framework: the problem is how to design an unsupervised learning task over the graph for pre-training the GNN model
* How to get θ^* = max_θ p(G; θ)
* Most existing graph generation methods follow auto-regressive manner to factorize the probability objective:
  * i.e: nodes come in an order and edges are generated by connecting new arriving nodes to existing nodes
  * If we have a permutation vector pi, the target graph distribution can be equivalent to the expected likelihood over all permutations
  * <img width="240" alt="Screenshot 2022-04-04 at 14 17 02" src="https://user-images.githubusercontent.com/598891/161552146-2b0e03b2-0926-4544-8cdc-d88d04cb00f3.png">
  * Given a permutated order, we cna factorize the log-likelihood autoregressively generating one node per iteration:
  * <img width="376" alt="Screenshot 2022-04-04 at 14 17 54" src="https://user-images.githubusercontent.com/598891/161552313-ff51f24b-94ba-4354-81e8-66c8b2e72b28.png">
* <img width="1090" alt="Screenshot 2022-04-04 at 14 18 34" src="https://user-images.githubusercontent.com/598891/161552433-2b859594-aa89-4af4-bbc1-839ac265ac49.png">
